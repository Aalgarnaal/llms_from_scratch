{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07edc5e1-a9dd-47e2-b0b0-59f734a9af40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e4b1362-1bf2-4005-94e7-98b9cdb60cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# opening the file in read mode \n",
    "my_file = open(\"../data/names.txt\", \"r\") \n",
    "# reading the file \n",
    "data = my_file.read()\n",
    "# split the names into a list\n",
    "name_list = data.split('\\n')\n",
    "name_list = [\".\"+x for x in name_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1eb0db7f-dc00-4441-85bc-a42a270e923d",
   "metadata": {},
   "outputs": [],
   "source": [
    "chartoix = {char: ix for ix, char in enumerate(sorted(set(\"\".join(name_list))))}\n",
    "ixtochar = {ix: char for ix, char in enumerate(sorted(set(\"\".join(name_list))))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c33578f-1097-4af4-9a92-6d97418a61fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_x_y(name_list):\n",
    "    # Create a dataset with, for each example, all possible X and Y combinations we can extract from it\n",
    "    block_size = 3\n",
    "    y_list = []\n",
    "    x_list = []\n",
    "    for name in name_list:\n",
    "        for target_ix, target_letter in enumerate(name):\n",
    "            previous_letters = name[max(-(block_size)+target_ix,0):target_ix] # This will extract the previous 3 letters\n",
    "            num_pads = block_size - len(previous_letters)\n",
    "            previous_letters = \".\"*num_pads + previous_letters\n",
    "            #print(previous_letters + \"-->\" + target_letter)\n",
    "            y_list.append(chartoix[target_letter])\n",
    "            x_list.append([chartoix[input_letter] for input_letter in previous_letters])\n",
    "            \n",
    "    X = torch.tensor(x_list) # Tensor with 3 dimensional list, input letter index\n",
    "    Y = torch.tensor(y_list) # Tensor with 1 dimensional output\n",
    "    return X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "895da972-9b6a-4700-9da7-f184d346bf3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ix_80_pct = int(0.8*len(name_list))\n",
    "ix_90_pct = int(0.9*len(name_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14aa1988-2b32-4bf9-b171-9fad87b9c25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train = build_x_y(name_list[:ix_80_pct])\n",
    "X_val, Y_val= build_x_y(name_list[ix_80_pct:ix_90_pct])\n",
    "X_test, Y_test = build_x_y(name_list[ix_90_pct:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71315a38-4b26-4b32-981d-a8183ede7084",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([182778, 3]),\n",
       " torch.Size([182778]),\n",
       " torch.Size([22633, 3]),\n",
       " torch.Size([22633]),\n",
       " torch.Size([22735, 3]),\n",
       " torch.Size([22735]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, Y_train.shape, X_val.shape, Y_val.shape, X_test.shape, Y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ad39baac-c9eb-4865-abf0-b190efac3ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Parameters ########\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "C = torch.randn(27,10)\n",
    "W1 = torch.randn(30,300)\n",
    "#b1 = torch.randn(300) not needed when doing batch norm right after!\n",
    "# Initializing W2 and b2 with scaled down values ensures we start with a reasonable loss.\n",
    "W2 = torch.randn(300,27) * 0.1\n",
    "b2 = torch.randn(27) * 0\n",
    "bngain = torch.ones((1,300))\n",
    "bnbias = torch.zeros((1,300))\n",
    "bnmean_running = torch.zeros((1,300))\n",
    "bnstd_running = torch.ones((1,300))\n",
    "parameters = [C, W1, W2, b2, bngain, bnbias]\n",
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cadb7902-f30a-4aac-bcbd-26ded6c0e51f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.2744598388671875\n",
      "2.597407579421997\n",
      "2.447227954864502\n",
      "2.5438640117645264\n",
      "2.341860294342041\n",
      "2.563021183013916\n",
      "1.8882859945297241\n",
      "2.348989725112915\n",
      "2.6667160987854004\n",
      "2.5552005767822266\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "steps = []\n",
    "\n",
    "for i in range(1000):\n",
    "    ###### Minibatch #######\n",
    "    minibatch_size=32\n",
    "    minibatch_ix = torch.randint(0,len(X_train),(minibatch_size,))\n",
    "    \n",
    "    ###### Forward pass ########\n",
    "    embs = C[X_train[minibatch_ix]]\n",
    "    h = torch.tanh(embs.view(-1,30) @ W1) #+ b1) #embs.view will reshape. No need to have b1 when doing batch norm.\n",
    "\n",
    "    # h is calculated to be shape 32,300 (32 examples).\n",
    "    batch_mean_i = h.mean(0,keepdims=True) # 1,300 mean over all elements in the batch\n",
    "    batch_std_i = h.std(0,keepdims=True) # 1,300 mean over all elements in the batch\n",
    "    \n",
    "    # Keep running tally of the batch norm statistics. This is seperate from the optimization\n",
    "    with torch.no_grad():\n",
    "        bnmean_running = 0.999*bnmean_running + 0.001*batch_mean_i\n",
    "        bnstd_running = 0.999*bnstd_running + 0.001*batch_std_i\n",
    "        \n",
    "    # batch norm\n",
    "    h = bngain*(h-batch_mean_i)/(batch_std_i+1e-5) + bnbias #now, every neuron in its firing rate will be unit gaussian over these 32 examples.\n",
    "\n",
    "\n",
    "        \n",
    "    # keep running mean of the values\n",
    "    logits = h @ W2 + b2\n",
    "    loss = F.cross_entropy(logits,Y_train[minibatch_ix]) #This makes forward/backward pass more efficient and makes things more well behaved numerically for large vals.\n",
    "\n",
    "    ###### Backward pass ########\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "    lr = 0.1 if i < 100000 else 0.01\n",
    "    for p in parameters:\n",
    "        p.data += -lr*p.grad\n",
    "    if i % 100 == 0:\n",
    "        print(loss.item())\n",
    "    #steps.append(i)\n",
    "    #losses.append(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c1459e36-fc09-475f-bba2-85c082633bd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.546466827392578\n"
     ]
    }
   ],
   "source": [
    "# validation loss\n",
    "embs_val = C[X_val]\n",
    "h_val = torch.tanh(embs_val.view(-1,30) @ W1)\n",
    "h_val = bngain*(h_val-bnmean_running)/(bnstd_running) + bnbias #now, every neuron in its firing rate will be unit gaussian over these 32 examples.\n",
    "logits_val = h_val @ W2 + b2\n",
    "val_loss = F.cross_entropy(logits_val,Y_val) #This makes forward/backward pass more efficient and makes things more well behaved numerically for large vals.\n",
    "print(val_loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7710ae-9646-461d-a229-cd1caeac4546",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa848cc-afcd-43ab-a1eb-26902840e8f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
